---
classes: wide2
title: "ä»é›¶å®ç° LLM Trainingï¼š023. Gradient Accumulation and Clip Gradient Norm"
excerpt: "å®ç°æ¢¯åº¦ç´¯ç§¯å’Œæ¢¯åº¦è£å‰ªï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚"
categories: 
  - LLM
  - Training
tags: 
  - LLM
  - Training
toc: true
toc_sticky: true
mathjax: true
---

æœ¬æ–‡ PR ä¸»è¦æ¥å®ç°å‡ ä¸ªå’Œæ¢¯åº¦ç›¸å…³çš„éå¸¸é‡è¦åŠŸèƒ½ï¼š

* gradient accumulation
* clip gradient norm

é¦–å…ˆç¬¬ä¸€ä¸ªä¸»è¦æ˜¯ç”¨æ¥æ‰©å…… effective batch size çš„ï¼Œæ¯”å¦‚æˆ‘çš„ GPU çš„æ˜¾å­˜å°çš„å¯æ€œï¼Œæ¯æ¬¡åªèƒ½å¡çš„ batch size ä¸º 1ï¼Œä½†æ˜¯æˆ‘åˆæƒ³æœ‰æ¯”è¾ƒå¤§çš„ batch size ä»è€Œèƒ½å¤Ÿä½¿æ¢¯åº¦çš„å™ªå£°ä¸è‡³äºé‚£ä¹ˆå¤§ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡å¤šæ¬¡å‰å‘åå‘åå†åšä¸€æ¬¡ optimizer stepï¼ˆè€Œä¸æ˜¯æ¯æ¬¡å‰å‘åå‘åç›´æ¥åš optimizer stepï¼‰ï¼Œè¿™é‡Œéœ€è¦æ³¨æ„æœ‰ä¸€ä¸ª loss /= grad_accum_steps å› ä¸ºæˆ‘ä»¬æ˜¯ç§¯ç´¯å¤šä¸ª steps çš„æ¢¯åº¦ã€‚

å¦ä¸€ä¸ªåˆ™æ˜¯ clip gradient normï¼Œä¹Ÿå°±æ˜¯å¯¹æ¢¯åº¦èŒƒæ•°è¿›è¡Œè£å‰ªï¼Œæ¢¯åº¦èŒƒæ•°å®é™…ä¸Šå°±æ˜¯æ¢¯åº¦é‡Œé¢æ‰€æœ‰æ•°å„è‡ªçš„å¹³æ–¹åŠ èµ·æ¥ï¼Œç„¶åæœ€åå¼€æ ¹å·ï¼ŒèŒƒæ•°ç›´è§‚æ„ä¹‰ä¸Šå°±æ˜¯è¡¨ç¤ºè¿™ä¸ªæ¢¯åº¦æ•´ä½“æ•°å€¼å¤§æ¦‚æ˜¯å¤šå¤§ï¼Œç†æƒ³æƒ…å†µä¸‹åº”è¯¥æ˜¯åœ¨ 1 ä»¥å†…ï¼Œé˜²æ­¢è¿‡å¤§å¯¼è‡´è®­å´©ï¼Œæ‰€ä»¥ä¸€èˆ¬åšæ¢¯åº¦èŒƒæ•°ä¸º 1 çš„æ¢¯åº¦èŒƒæ•°è£å‰ªï¼Œè£å‰ªçš„æ—¶å€™è¦æ³¨æ„å¦‚æœä½¿ç”¨äº† AMP çš„è¯éœ€è¦å…ˆè°ƒç”¨ scaler.unscale_(optimizer) æ¥å»æ‰ AMP çš„ scale æ“ä½œã€‚

## æ ¸å¿ƒä»£ç å˜æ›´

![image-20251202201216928](https://cdn.jsdelivr.net/gh/WineChord/typora-images/img/image-20251202201216928.png)

![image-20251202201246515](https://cdn.jsdelivr.net/gh/WineChord/typora-images/img/image-20251202201246515.png)

train_ddp.py çš„ä»£ç å˜æ›´ç±»ä¼¼ã€‚

## è¿è¡Œ

```shell
(/data/projects/rosellm/.conda) wine@wine-MS-7D90:/data/projects/rosellm/rosellm/rosetrainer$ ./train_gpt2_small_minimal.sh 
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: guoqizhou123123 (guoqizhou123123-tencent) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /data/projects/rosellm/rosellm/rosetrainer/wandb/run-20251202_200347-tlne3zj3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sunset-7
wandb: â­ï¸ View project at https://wandb.ai/guoqizhou123123-tencent/rosetrainer
wandb: ğŸš€ View run at https://wandb.ai/guoqizhou123123-tencent/rosetrainer/runs/tlne3zj3
[2025-12-02 20:03:48] Training started at 2025-12-02 20:03:48
[2025-12-02 20:03:48] Using device: cuda
[2025-12-02 20:03:48] Arguments: Namespace(vocab_size=50257, max_position_embeddings=1024, n_layers=12, n_heads=12, d_model=768, d_ff=3072, dropout=0.1, use_tensor_parallel=False, use_activation_checkpoint=False, batch_size=2, seq_len=1024, num_steps=50, lr=0.0003, no_amp=False, checkpoint_path='checkpoints/gpt2_small_minimal.pt', resume=False, lr_scheduler='cosine', warmup_steps=100, use_profiler=False, seed=42, grad_accum_steps=2, grad_clip_norm=1.0, train_data=['data/train.txt'], val_data=[], tokenizer_name='gpt2', use_toy_data=False, max_tokens=100000, data_seed=42, data_mode='text', train_npy=[], val_npy=[], use_wandb=True, wandb_project='rosetrainer', wandb_run_name=None)
total files: 1
total tokens: 100000
[2025-12-02 20:03:50] train dataset size: 88
[2025-12-02 20:03:50] val dataset size: 9
[2025-12-02 20:03:50] steps per epoch: 44
[2025-12-02 20:03:51] Starting from scratch
[2025-12-02 20:03:54] ('epoch 1 step 10 / 50 ', 'lr: 0.000033 ', 'step time: 0.29s ', 'tokens/sec: 14060.44 ', 'grad norm: 2.2948 ', 'train loss: 4.9909 ', 'val loss: 9.8618 ', 'val ppl: 19183.9024 ', 'dt: 3.27s ', 'eta: 0.00h ', 'amp: True')
[2025-12-02 20:03:59] ('epoch 1 step 20 / 50 ', 'lr: 0.000063 ', 'step time: 0.29s ', 'tokens/sec: 14042.51 ', 'grad norm: 2.7618 ', 'train loss: 4.3298 ', 'val loss: 8.5735 ', 'val ppl: 5289.3874 ', 'dt: 4.73s ', 'eta: 0.00h ', 'amp: True')
[2025-12-02 20:04:02] ('epoch 2 step 30 / 50 ', 'lr: 0.000093 ', 'step time: 0.29s ', 'tokens/sec: 14045.78 ', 'grad norm: 2.6866 ', 'train loss: 3.7137 ', 'val loss: 7.3583 ', 'val ppl: 1569.2035 ', 'dt: 3.11s ', 'eta: 0.00h ', 'amp: True')
[2025-12-02 20:04:07] ('epoch 2 step 40 / 50 ', 'lr: 0.000123 ', 'step time: 0.29s ', 'tokens/sec: 14046.11 ', 'grad norm: 1.9740 ', 'train loss: 3.0307 ', 'val loss: 5.7499 ', 'val ppl: 314.1686 ', 'dt: 4.80s ', 'eta: 0.00h ', 'amp: True')
[2025-12-02 20:04:10] ('epoch 3 step 50 / 50 ', 'lr: 0.000153 ', 'step time: 0.29s ', 'tokens/sec: 14046.43 ', 'grad norm: 1.2691 ', 'train loss: 2.2031 ', 'val loss: 4.3959 ', 'val ppl: 81.1191 ', 'dt: 3.11s ', 'eta: 0.00h ', 'amp: True')
[2025-12-02 20:04:10] Training finished.
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            amp â–â–â–â–â–
wandb:      grad_norm â–†â–ˆâ–ˆâ–„â–
wandb:             lr â–â–ƒâ–…â–†â–ˆ
wandb:      step_time â–â–ˆâ–‡â–‡â–†
wandb: tokens_per_sec â–ˆâ–â–‚â–‚â–ƒ
wandb:     train/loss â–ˆâ–†â–…â–ƒâ–
wandb:       val/loss â–ˆâ–†â–…â–ƒâ–
wandb:        val/ppl â–ˆâ–ƒâ–‚â–â–
wandb: 
wandb: Run summary:
wandb:            amp 1
wandb:      grad_norm 1.26913
wandb:             lr 0.00015
wandb:      step_time 0.2916
wandb: tokens_per_sec 14046.43304
wandb:     train/loss 2.20305
wandb:       val/loss 4.39592
wandb:        val/ppl 81.11907
wandb: 
wandb: ğŸš€ View run northern-sunset-7 at: https://wandb.ai/guoqizhou123123-tencent/rosetrainer/runs/tlne3zj3
wandb: â­ï¸ View project at: https://wandb.ai/guoqizhou123123-tencent/rosetrainer
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251202_200347-tlne3zj3/logs
(/data/projects/rosellm/.conda) wine@wine-MS-7D90:/data/projects/rosellm/rosellm/rosetrainer$ ./train_gpt2_small_ddp.sh 
W1202 20:04:22.473000 2780229 site-packages/torch/distributed/run.py:792] 
W1202 20:04:22.473000 2780229 site-packages/torch/distributed/run.py:792] *****************************************
W1202 20:04:22.473000 2780229 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 20:04:22.473000 2780229 site-packages/torch/distributed/run.py:792] *****************************************
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: guoqizhou123123 (guoqizhou123123-tencent) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /data/projects/rosellm/rosellm/rosetrainer/wandb/run-20251202_200425-q6y5u4li
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-wood-8
wandb: â­ï¸ View project at https://wandb.ai/guoqizhou123123-tencent/rosetrainer
wandb: ğŸš€ View run at https://wandb.ai/guoqizhou123123-tencent/rosetrainer/runs/q6y5u4li
[2025-12-02 20:04:26] Training started at 2025-12-02 20:04:26
[2025-12-02 20:04:26] [rank 0] Using device: cuda:0
[2025-12-02 20:04:26] Arguments: Namespace(vocab_size=50257, max_position_embeddings=1024, n_layers=12, n_heads=12, d_model=768, d_ff=3072, dropout=0.1, use_tensor_parallel=False, use_activation_checkpoint=False, batch_size=2, seq_len=1024, num_steps=50, lr=0.0003, no_amp=False, checkpoint_path='checkpoints/gpt2_small_ddp.pt', resume=False, lr_scheduler='cosine', warmup_steps=100, use_profiler=False, seed=42, grad_accum_steps=2, grad_clip_norm=1.0, train_data=['data/train.txt'], val_data=[], val_ratio=0.001, data_mode='text', train_npy=[], val_npy=[], tokenizer_name='gpt2', use_toy_data=False, max_tokens=100000, data_seed=42, use_wandb=True, wandb_project='rosetrainer', wandb_run_name=None)
total files: 1
total tokens: 100000
total files: 1
total tokens: 100000
[2025-12-02 20:04:28] train dataset size: 96
[2025-12-02 20:04:28] val dataset size: 1
[2025-12-02 20:04:28] steps per epoch: 24
[2025-12-02 20:04:28] [rank 0] Starting from scratch
[2025-12-02 20:04:34] ('epoch 1 step 10 / 50 ', 'lr: 0.000033 ', 'step time: 0.55', 'toks/s (per rank): 7396.99', 'grad norm: 2.0187 ', 'train loss: 5.0777 ', 'val loss: 9.8426 ', 'val ppl: 18817.6630 ', 'dt: 5.77s ', 'eta: 0.01h ', 'amp: True')
[2025-12-02 20:04:41] ('epoch 2 step 20 / 50 ', 'lr: 0.000063 ', 'step time: 0.55', 'toks/s (per rank): 7419.99', 'grad norm: 2.6765 ', 'train loss: 4.2946 ', 'val loss: 8.5512 ', 'val ppl: 5172.7062 ', 'dt: 7.24s ', 'eta: 0.01h ', 'amp: True')
[2025-12-02 20:04:47] ('epoch 3 step 30 / 50 ', 'lr: 0.000093 ', 'step time: 0.55', 'toks/s (per rank): 7405.65', 'grad norm: 2.5235 ', 'train loss: 3.6666 ', 'val loss: 7.2297 ', 'val ppl: 1379.7863 ', 'dt: 5.55s ', 'eta: 0.00h ', 'amp: True')
[2025-12-02 20:04:54] ('epoch 4 step 40 / 50 ', 'lr: 0.000123 ', 'step time: 0.55', 'toks/s (per rank): 7385.33', 'grad norm: 1.8517 ', 'train loss: 2.8361 ', 'val loss: 5.6246 ', 'val ppl: 277.1675 ', 'dt: 7.24s ', 'eta: 0.00h ', 'amp: True')
[2025-12-02 20:05:00] ('epoch 5 step 50 / 50 ', 'lr: 0.000153 ', 'step time: 0.55', 'toks/s (per rank): 7426.97', 'grad norm: 1.2541 ', 'train loss: 2.1491 ', 'val loss: 4.2860 ', 'val ppl: 72.6787 ', 'dt: 5.55s ', 'eta: 0.00h ', 'amp: True')
[2025-12-02 20:05:00] Training finished.
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                     amp â–â–â–â–â–
wandb:   global_tokens_per_sec â–ƒâ–‡â–„â–â–ˆ
wandb:               grad_norm â–…â–ˆâ–‡â–„â–
wandb:                      lr â–â–ƒâ–…â–†â–ˆ
wandb: tokens_per_sec_per_rank â–ƒâ–‡â–„â–â–ˆ
wandb:              train/loss â–ˆâ–†â–…â–ƒâ–
wandb:                val/loss â–ˆâ–†â–…â–ƒâ–
wandb:                 val/ppl â–ˆâ–ƒâ–â–â–
wandb: 
wandb: Run summary:
wandb:                     amp 1
wandb:   global_tokens_per_sec 14853.93592
wandb:               grad_norm 1.25406
wandb:                      lr 0.00015
wandb: tokens_per_sec_per_rank 7426.96796
wandb:              train/loss 2.14906
wandb:                val/loss 4.28605
wandb:                 val/ppl 72.67867
wandb: 
wandb: ğŸš€ View run still-wood-8 at: https://wandb.ai/guoqizhou123123-tencent/rosetrainer/runs/q6y5u4li
wandb: â­ï¸ View project at: https://wandb.ai/guoqizhou123123-tencent/rosetrainer
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251202_200425-q6y5u4li/logs
(/data/projects/rosellm/.conda) wine@wine-MS-7D90:/data/projects/rosellm/rosellm/rosetrainer$ 
```

